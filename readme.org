#+title: DERP: Deep recursive parameter estimation
#+author: Alexander E. Zarebski

This repository contains the code to simulate a database of
phylogenetic trees that will be used in a machine learning project in
which a neural network will be trained to solve phylodynamics
problems.

* Roadmap

- [-] Provide a single database file and some helper functions to read
  trees out and their associated metadata.
  + [-] There should be a bunch of visualisation so that we can get a
    feel for if the parameters of the simulation are at all plausible.
    * [ ] Use a =visualisation.org= file in the same way as the
      training repository.
    * [X] Visualisation of change points
    * [X] Histogram of the last sequenced sample time
    * [ ] Histogram of prevalence
    * [ ] Histogram of cumulative infections
    * [ ] A single simulation visualisation showing the LTT and the
      rates through time.
  + [X] There should be a file to specify the conda environment used
    and instructions on how to recreate this.
  + [X] Establish the prior distribution to draw simulation parameters
    from, these will need to be sufficiently large to cover the range
    of plausible values for the use cases.
  + [ ] Estimate an appropriate size of the database, we can use
    10,000 as a working number of samples.
  + [X] This should be robust to failed simulations and include basic
    metadata of the simulation such as the dataset size and the date
    it was created.
  + [X] Store the trees and metadata in a single database file.
    * [X] Write a function that simulates the simulation parameters:
      - number of rate changes
      - duration of epidemic
      - birth, death and sampling rates
    * [X] Write a function that takes the metadata and simulates an
      epidemic with remaster.
      - [X] the remaster template file to run the simulation
      - [X] the code to load the simulation parameters into the
        template
    * [X] Write a function that processes the epidemic simulation to
      extract the desired features for input/output.
    * [X] Split the creation of the intermediate pickle files across
      multiple workers in the same way we split the BEAST2
      simulations.
    * [X] Write a function that loads all the data from one simulation
      into the database.
  + [X] The simulation should be configured by a plain text file so
    that it is easy to toggle between a small debugging simulation and
    a large one for downstream use.
    * [X] The configuration file sould be the only command line
      argument taken by =main.py=
    * [X] The visualisation script should probably have the
      configuration script hardcoded since this will usually be run
      interactively and in this case it should be easy enough to
      change it if necessary.
    * [X] The configuration should include some of the hyperparameters
      of the simulation:
      - [X] Range of epidemic duration.
      - [X] Upper limit on the number of change points.
  + [ ] Document how to use the database file.
    * [ ] There should be a helper function which explains how to read
      trees and their corresponding record out.

* Usage

** Building the database

To generate the database, run the main script.

#+begin_src sh
 python main.py <path/to/config.json>
#+end_src

** Using the database

The following demonstrates how to use the database. Don't forget to
close the database connection after using it!

#+begin_src python
db_conn = h5py.File("dataset.hdf5")
for k in db_conn.keys():
    out_grp = db_conn[k]['output']
    r0_vals = out_grp['parameters']['r0']['values'][()]
    r0_chng = out_grp['parameters']['r0']['change_times'][()]
    prev = out_grp["present_prevalence"][()]
    print(f"Record {k} prevalence {prev}")
    tree = pickle.loads(db_conn[k]['input']['tree'][...].tobytes())
db_conn.close()
#+end_src

** Conda environment

A conda environment to run this simulation can be created from the
=environment.yaml= file by running the following command:

#+begin_src sh
  conda env create -f environment.yaml
#+end_src

* Notes

1. Activate the =derp= environment in conda and run =python main.py
   config/debugging.json= to run the debugging example. This will
   produce a bunch of pickle files, each containing a single record of
   the dataset and a HDF5 file which contains the pickled trees as
   binary blobs and the various parameters and statistics that we
   might be interested in estimating from those trees. The relevant
   files are all defined at the start of =main.py= as global variables
   read from the configuration JSON file.
2. To set up BEAST2 to do the simulation you can run the
   =src/setuplib.sh= script which will download BEAST2. Run =python
   clean.py= to remove output to start again fresh. Run =bash
   src/housekeeping.sh= to update =environment.yaml= and lint the
   code.
3. If you want a GUI to inspect the output HDF5 file, the [[https://github.com/HDFGroup/hdf-compass][HDFCompass]]
   tool provides a simple way to inspect the data that has been
   generated. There is some basic information about the simulation
   stored as attributes in the HDF5 file. This includes the date of
   creation and the size of the dataset.
4. There are a sequence of configurations: /Charmander/, /Charmeleon/
   and /Charizard/. These all use the same model but are of increasing
   size and use broader distributions over the simulation parameters.
   - Charmander :: has a 600-100-100 training-validation-testing split
     with short simulations and only slightly varying parameters. The
     net removal rate only takes a very small range of values to avoid
     potential identifiability issues.
